---
layout: post
title:  "Ligra"
date:   2016-07-09
categories: paper-series graph-processing pram parallel
---

# Ligra: A Lightweight Graph Processing Framework for Shared Memory

**Published** Feburary 2013

**Read** July 2016

# Abstract

Ligra graph processing goals:

* Single machine, shared memory, multicore
* Lightweight
* BFS
* Mapping over vertex subsets
* Mapping over edge subsets
* Adapt to varying vertex degrees

# Introduction

##### Motivation for Single Machine Framework

Lower communication cost allows for performance benefits.

Current demands do not necessitate the distributed computation framework previous implementations provide:

1. Largest non-synthetic dataset has <7B edges
2. All papers except Pregel use <20B edge datasets, Pregel uses <130B.

High-end hardware reaches 64 cores and 2TB of data. With near-linear speedup that Ligra offers, this doesn't suffer from issues that distributed.

##### Capabilities

Ligra offers a few primitives to operate on graphs.

###### Data Types

Graphs are represented by the vertex set and edge set, where the vertex set is a set of indices. Vertex subsets are represented with the fundamental accounting type in Ligra: the \\(\textbf{vertexSet}\\).

A \\(vertexSubset\\), \\(S\\), may be represented sparsely or densely, and answers the question \\(v \in S\\) for verices \\(v\\).

###### Operations

(TODO: copy section 4.1 directly)

##### Differentiation

The above interface allows for iteration over out-edges in parallel. The order in which the out-edges are processed is decided by the vertex sets that are passed in through successive calls to the \\(\tt{edgeMap}\\) method.

As such, it's the derivation of the \\(vertexSubset\\)s that determines the linearization points for the algorithm's computation: the sets are a method of **control flow** as much as they hold data, much like lists in Haskell.

##### Proof of Concept

Using the above primitives, Ligra authors implemented:
1. BFS
1. Betweenness centrality
1. Graph radii estimation
1. Graph connectivity
1. PageRank
1. Bellman-Ford single-source shortest paths.

##### Questions

* Does no order over out-edge calling prevent certain algos?
* How does Ligra compare relative to distributed frameworks, if the distributed frameworks are given enough compute? (TODO: look up 2013 baselines for Yahoo, etc -> some in paper) - ultimately we want it done.
* This was published in 2013. Do those TB assumptions still hold now? In terms of existing data sets, the answer is still yes - [most datasets people work with](http://www.kdnuggets.com/2015/11/big-ram-big-data-size-datasets.html) are well under the 2TB cutoff. However, this may be due to simply what people _can_ handle, not what they want to. Certainly with ad-tech and web related data graphs can easily scale above 10TB range. Can we apply the framework in some parallel fashion?
* How are reduce-like operations performed? Need to do CAS / atomic add? What about more complicated reduces - would I need to lock? How does this affect perf?
  -> 

## Related Work


Why does Pregel-like computation suck?
1. Too restrictive
1. With the recent exception of GraphLab, there is no way to loop over each vertex's out edges in parallel for processing, which is a problem for high-degree vertices.

What are the limitations of GraphLab? Single-graph computation only: no multiple vertex subsets (bidirectional search, forward-backward search disabled).

Pregel: associative reduces.

##### Notes

* By enabling the user to maintain active vertex subsets, we don't need to do expensive no-ops on the entire graph like the distributed frameworks do as they near convergence in iterative computations - can some frameworks like Spark take advantage of such notions of dynamic vertex sets?
* What about the shared memory architecture is essential to Ligra? What prevents a smart distributed system from taking the vertex subset model?
  -> need to have variable vertex sets (feasible), but also mutable vertex state -> can be done, not in current spark versions w/ immutability

## Preliminaries

(skipped)

## Framework

### Interface

(see [Introduction.Capabilities](#capabilities))

### Implementation

##### Vertex Subset Representation

Sparse representation is an unordered list of vertex indices. (TODO: why not sorted? Querying contains is slow...)

Dense representation is a boolean array of size \\(\left|V\right|\\).

##### Edge Map

algs 1, 2, 3, 5 (TODO add to below, clean up below, add work + linear pre/post work + span)

edgemap - how to compute sum of out-degrees quickly?
eagerly uses dense representation (output could be below threshold)

edgemapsparse - input/output sparse, requires dedup (TODO: would've been faster sorted parallel set) (TODO: alternatively, can keep dups and just use an atomic flag for whether processed in the next step to do always-once guarantee - only allows one edgeMap at a time, prevents immutability)
edgemapdense -  output is dense. Note dense uses indegree. 

Threshold of |E|/20 - best determined by looking at Amdahl's law and finding optimal, but dependent on F runtime and how selective C is.


Does "add i to out" have to be atomic -> for bitset, yes... TODO 

(TODO- why indegree?)

discussion: should usually be sparse -> sparse and dense-> dense transitions (expensive to do sparse-> dense because slow member checking - would be easy to optimize in sparkse->dense case by presorting sparse input list.)

##### Vertex Map
alg 4. Representation?

### Graph Representation

Edges stored in a one-dimensional array. Vertices own partitions of this arra, so a vertex array delimits vertex partitions. Thus, edge array only needs to store the target vertex and vertex array only needs to store one endpoint. Very efficient memory of \\(i(\left|V\right| + \left|E\right|)\\) where we use \\(i\\) bytes per index.

### Optimizations

(TODO: describe inline with impls)

## Applications

(TODO: complete)

## Experiments

(TODO: include Table 2)

(TODO summarize discussion)

(TODO: figure 2)

## Conclusion

Certain algorithms require graph modifications, which is not currently feasible. However, this isn't achieved with the distributed frameworks and the problem is hard to pose with concurrent graph processing.

GPU future work is posited, but how feasible it? (TODO: do similar atomics exist for GPUs? What are the typical functions applied F? Do they work well with GPUs?)
 (TODO diff: can't modify graph due to extremely static representation, but can with true adjacency in spark)

(Make new buffers every time)
https://github.com/jshun/ligra/blob/master/ligra/ligra.h#L78
